# **1. 기본 타임라인 설명**
### 1-1. **SMT의 단점:**
- 복잡도가 증가하고 여러 모델이 생성될 수 있다.
- n-gram으로 인한 희소성 문제 발생
- 그래서 nmt 등장

###1-2. **NMT란:**
- nmt는 최근 제안한 기계 번역에 대한 새롭 게 떠오르는 접근 방식
- NMT는 인코더와 디코더로 분리된 매우 작은 서브 컴포넌트 및 구성 요소로 이루어져 있음.
- 인코더, 디코더는 구조로 분리되어 있음.
- 인코더 신경망은 소스 문장을 읽고 고정 길이 벡터로 인코딩. 그런 다음 디코더는 인코딩된 벡터에서 번역을 출력
- conditional probability를 최대화하여 정확도 향상이 목표

### 1-3. **NMT(인코더, 디코더 방식)의 문제:**
- 고정 길이 벡터가 정보를 압축하는 데 정보 손실 위험이 크며, 문장이 길어질수록 성능이 저하됨.
- 실제로 기본 인코더-디코더의 성능은 입력 문장의 길이 가 증가함에 따라 급격히 저하되는 것으로 나타남

### 1-4. **해결**
- 이 문제를 해결하기 위해 논문의 저자는 공동으로 정렬하고 변환하는 방법을 학습하는 인코더-디코더 모델의 확장을 도입
- 번역에서 단어를 생성할 때마다 원본 문장에서 가장 관련성이 높은 정보가 집중 된 위치 집합을 (소프트) 검색. 그런 다음 모델은 이러한 소스 위치 및 이전에 생성된 모든 대상 단 어와 관련된 컨텍스트 벡터를 기반으로 대상 단어를 예측
- 어프로치: 인코더 어텐션 디코더 구조를 사용.
- 바이디렉셔널 RNN을 활용하여 히든 스테이트를 두 단어로 구성.
- 어텐션을 계산하고 디코더에 전달하기 위해 중간에 특별한 "ATTENTION" 부분을 도입.

  #**2. NMT, RNN Encoder, Decoder**

  ###2-1. NMT
- 번역을 할 때 조건부 확률이 가장 높은 y(결과)를 찾아야 함
- nmt에서는 매개변수화된 모델을 적용하여 병렬 훈련 코퍼스를 사용하여 문장 쌍의 조건부 확률을 최대화하는 문장을 찾아 생성
- 인코더-디코더 구조로 주로 이루어짐
- 입력 문장 x를 인코딩, 출력 문장 y로 디코딩
- (Cho et al., 2014a)와 (Sutskever et al., 2014)에서 RNN을 사용하여 가변 길이의 소스 문장을 고정 길이 벡터로 인코딩하고 이를 가변 길이의 대상 문장으로 디코딩


### 2-1-1. 의의
- 새로운 접근 방식이지만, 신경 기계 번역은 이미 훌륭한 결과를 보여주고 있다. Sutskever 등(2014)은 RNN(순환 신경망)과 장단기 메모리(LSTM) 유닛을 기반으로 한 신경 기계 번역이 전통적인 구문 기반 기계 번역 시스템과 유사한 성능을 달성한다고 보고
- 신경 구성 요소를 기존 번역 시스템에 추가하는 것, 예를 들어 구(phrase) 쌍을 점수화하기 위해 (Cho et al., 2014a)나 후보 번역을 다시 순위 지정하기 위해 (Sutskever et al., 2014) 같은 방식으로, 기존의 최첨단 성능 수준을 뛰어넘음


###2-2. RNN encoder decoder


---

**Encoder-Decoder 프레임워크:**

Encoder-Decoder 프레임워크에서 인코더는 입력 문장, 즉 벡터의 시퀀스 x = (x₁, ..., xₙ)를 벡터 c로 읽어들임. 가장 흔한 방법은 RNN을 사용하는 것이며, 이를 통해 다음과 같이 표현됨:

- hₜ = f(xₜ, hₜ₋₁)  (1)
- c = q({h₁, ..., hₙ})

여기서 hₜ ∈ ℝⁿ은 시간 t에서의 hidden state이며, c는 hidden state의 시퀀스에서 생성된 벡터입니다. f와 q는 어떤 비선형 함수를 나타냄. 예를 들어, Sutskever 등(2014)은 f로 LSTM을 사용하였으며, q는 q({h₁, ..., hₙ}) = hₙ와 같이 구현됨.

---
