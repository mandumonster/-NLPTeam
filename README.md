# **1. 기본 타임라인 설명**
### 1-1. **SMT의 단점:**
- 복잡도가 증가하고 여러 모델이 생성될 수 있음.
- n-gram으로 인한 희소성 문제 발생
- 그래서 nmt 등장

###1-2. **NMT란:**
- nmt는 최근 제안한 기계 번역에 대한 새롭 게 떠오르는 접근 방식
- NMT는 인코더와 디코더로 분리된 매우 작은 서브 컴포넌트 및 구성 요소로 이루어져 있음.
- 인코더, 디코더는 구조로 분리되어 있음.
- 인코더 신경망은 소스 문장을 읽고 고정 길이 벡터로 인코딩. 그런 다음 디코더는 인코딩된 벡터에서 번역을 출력
- conditional probability를 최대화하여 정확도 향상이 목표

### 1-3. **NMT(인코더, 디코더 방식)의 문제:**
- 고정 길이 벡터가 정보를 압축하는 데 정보 손실 위험이 크며, 문장이 길어질수록 성능이 저하됨.
- 실제로 기본 인코더-디코더의 성능은 입력 문장의 길이 가 증가함에 따라 급격히 저하되는 것으로 나타남

### 1-4. **해결**
- 이 문제를 해결하기 위해 논문의 저자는 공동으로 정렬하고 변환하는 방법을 학습하는 인코더-디코더 모델의 확장을 도입
- 번역에서 단어를 생성할 때마다 원본 문장에서 가장 관련성이 높은 정보가 집중 된 위치 집합을 (소프트) 검색. 그런 다음 모델은 이러한 소스 위치 및 이전에 생성된 모든 대상 단 어와 관련된 컨텍스트 벡터를 기반으로 대상 단어를 예측
- 어프로치: 인코더 어텐션 디코더 구조를 사용.
- 바이디렉셔널 RNN을 활용하여 히든 스테이트를 두 단어로 구성.
- 어텐션을 계산하고 디코더에 전달하기 위해 중간에 특별한 "ATTENTION" 부분을 도입.


# **2. Background of NMT, RNN Encoder, Decoder**

### 2-1. NMT
- 번역을 할 때 조건부 확률이 가장 높은 y(결과)를 찾아야 함
- nmt에서는 매개변수화된 모델을 적용하여 병렬 훈련 코퍼스를 사용하여 문장 쌍의 조건부 확률을 최대화하는 문장을 찾아 생성
- 인코더-디코더 구조로 주로 이루어짐
- 입력 문장 x를 인코딩, 출력 문장 y로 디코딩
- (Cho et al., 2014a)와 (Sutskever et al., 2014)에서 RNN을 사용하여 가변 길이의 소스 문장을 고정 길이 벡터로 인코딩하고 이를 가변 길이의 대상 문장으로 디코딩


### 2-1-1. nmt의 의의
- 새로운 접근 방식이지만, 신경 기계 번역은 이미 훌륭한 결과를 보여주고 있다. Sutskever 등(2014)은 RNN(순환 신경망)과 장단기 메모리(LSTM) 유닛을 기반으로 한 신경 기계 번역이 전통적인 구문 기반 기계 번역 시스템과 유사한 성능을 달성한다고 보고
- 신경 구성 요소를 기존 번역 시스템에 추가하는 것, 예를 들어 구(phrase) 쌍을 점수화하기 위해 (Cho et al., 2014a)나 후보 번역을 다시 순위 지정하기 위해 (Sutskever et al., 2014) 같은 방식으로, 기존의 최첨단 성능 수준을 뛰어넘음


### 2-2. RNN encoder decoder


Encoder-Decoder 프레임워크에서 인코더는 입력 문장, 즉 벡터의 시퀀스 x = (x₁, ..., xₙ)를 벡터 c로 읽어들임. 가장 흔한 방법은 RNN을 사용하는 것이며, 이를 통해 다음과 같이 표현됨:

- hₜ = f(xₜ, hₜ₋₁)  (1)
- c = q({h₁, ..., hₙ})

여기서 hₜ ∈ ℝⁿ은 시간 t에서의 hidden state이며, c는 hidden state의 시퀀스에서 생성된 벡터입니다. f와 q는 어떤 비선형 함수를 나타냄. 예를 들어, Sutskever 등(2014)은 f로 LSTM을 사용하였으며, q는 q({h₁, ..., hₙ}) = hₙ와 같이 구현됨.



디코더는 주로 컨텍스트 벡터 c와 이전에 예측된 모든 단어 {y₁, ..., yₜ₀₋₁}을 고려하여 다음 단어 yₜ₀를 예측하기 위해 훈련됨. 다시 말하면, 디코더는 결합 확률을 순서대로 조건부로 분해하여 번역 y에 대한 확률을 정의:

```
p(y) = Πₜ₌₁ᵀ p(yₜ | {y₁, ..., yₜ₋₁}, c)   (2)
```

여기서 y는 번역을 나타내며, T는 번역 문장의 길이를 나타냄. 이것은 디코더가 각 시점에서 현재까지 예측된 단어들과 컨텍스트 벡터 c를 기반으로 다음 단어의 확률을 예측하는 방식을 나타냄.

RNN을 사용하면 각 조건부 확률은 다음과 같이 모델링:

```
p(yₜ | {y₁, ..., yₜ₋₁} , c) = g(yₜ₋₁, sₜ, c)   (3)
```

여기서 g는 비선형적이고 여러 층으로 구성될 수 있는 함수로, yₜ₋₁과 sₜ 및 c에 대한 확률을 출력함. 여기서 sₜ는 RNN의 hidden state. 다른 아키텍처, 예를 들어 RNN과 de-convolutional 신경망의 혼합이 가능하다는 것에 유의해야 함 (Kalchbrenner and Blunsom, 2013).



# **3.  LEARNING TO ALIGN AND TRANSLATE**
이 섹션에서는 신경 기계 번역을 위한 새로운 아키텍처를 제안한다. 이 새로운 아키텍처는 인코더로 양방향 RNN(Recurrent Neural Network)을 사용하며(3.2절), 디코딩 중에 번역을 수행하면서 소스 문장을 검색하는 디코더를 포함하고 있음(3.1절).


### 3.1 DECODER: GENERAL DESCRIPTION
조건부 확률 정의
새로운 모델 아키텍처에서는 각 조건부 확률을 다음과 같이 정의:

p(yᵢ | y₁, ..., yᵢ₋₁, x) = g(yᵢ₋₁, sᵢ, cᵢ)   (4)

여기서 sᵢ는 시간 i에 대한 RNN의 히든 상태로, 다음과 같이 계산됨:

sᵢ = f(sᵢ₋₁, yᵢ₋₁, cᵢ)

기존의 인코더-디코더 접근 방식과는 달리 (Eq. (2) 참조), 이곳에서는 확률이 각 대상 단어 yᵢ에 대해 별도의 컨텍스트 벡터 cᵢ에 조건이 걸려 있음에 주목해야 함.

컨텍스트 벡터 cᵢ는 입력 문장을 인코더가 매핑한 주석(annotations) 시퀀스 (h₁, ..., hₜₓ)에 의존합. 각 주석 hᵢ는 입력 시퀀스의 i번째 단어 주변의 부분에 중점을 둔 정보를 포함. 다음 섹션에서는 주석이 어떻게 계산되는지에 대한 자세한 설명을 제공.

그런 다음 컨텍스트 벡터 cᵢ는 이러한 주석들 hᵢ의 가중 합으로 계산:

cᵢ = ∑_{j=1}^{Tₓ} αᵢⱼhⱼ   (5)

각 주석 hⱼ의 가중치 αᵢⱼ는 다음과 같이 계산:

αᵢⱼ = exp(eᵢⱼ) / ∑_{k=1}^{Tₓ} exp(eᵢₖ)   (6)

여기서 

eᵢⱼ = a(sᵢ₋₁, hⱼ)

는 정렬 모델로, j 위치 주변의 입력과 i 위치에서의 출력이 얼마나 잘 일치하는지를 점수로 표현. 이 점수는 RNN의 히든 상태 sᵢ₋₁ (yᵢ를 생성하기 직전에)와 입력 문장의 j번째 주석 hⱼ에 기반.
